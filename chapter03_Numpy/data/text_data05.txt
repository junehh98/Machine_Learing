오라클 데이터베이스와 MS Access에 대한 데이터베이스 접속을 지원합니다. 데이터베이스에서 여러가지 쿼리를 통해 자유자재로 데이터를 추출하고 가공할 수 있도록 도와줍니다. 
대용량 실시간 데이터와 배치 데이터를 다양한 분석 도구를 이용하여 쉽고 빠르게 분석할 수 있는 In-Memory Computing 기반의 고성능 빅데이터 
일반적으로 빅데이터 시스템을 얘기하자면 다음과 같이 구성할 수 있을 듯 싶다. 다양한 채널을 통한 대량의 데이터 수집 작업, 그리고 수집된 데이터를 분석하여 연관된 데이터끼리 분류하는 작업, 분류된 데이터를 기반으로 분석, 정리하여 2차 데이터를 만드는 작업, 2차 데이터를 분석하여 최종 결과물을 만드는 작업으로 구성할 수 있을 것이다. 물론 2차 데이터를 기반으로 3차 데이터를 만들 수도 있고 2차 데이터를 만들기 위해 분류된 데이터를 직접 분석하여(2차 데이터의 생성 없이) 최종 결과물을 만들 수도 있을 것이다. 어찌되었던 수집과 분석, 처리, 결과 생성이라는 4단계가 빅데이터 시스템의 구성이라고 볼 수 있을 것이다.
빅데이터 시스템이라고 얘기하면 가장 많이 언급되는 솔루션은 다름아닌 하둡(Hadoop)이다. 하둡은 하둡 파일 시스템을 통한 데이터 수집 및 저장, 맵리듀스를 이용한 데이터 처리를 진행한다. 빅데이터 시스템의 특징이라고 한다면 정형화된 데이터가 아닌 비정형 데이터를 하나의 채널이 아닌 다양한 채널을 통해 소량이 아닌 다량의 데이터를 수집하여 처리하는데 있는데 하둡은 정형 데이터 뿐만이 아니라 비정형 데이터도 빨리 저장하며 처리할 수 있으며 게다가 오픈소스로 제공되기 때문에 커스터마이징 요소가 많아서 많은 빅데이터 솔루션 구축 기업들이 쉽게 접근해서 사용하고 있다. 그래서 빅데이터 솔루션이라고 한다면 대부분 하둡을 많이 얘기하며 많은 사람들이 그렇게 또 인식하고 있다.
하지만 앞서 얘기했던 것처럼 빅데이터 시스템의 구성이 수집과 분석, 처리, 결과 생성이라는 4단계로 구성된다고 했을 때 하둡은 수집과 처리를 맡아서 처리하는 하나의 파트라고 할 수 있을 것이다. 수집과 처리 사이에는 분석이라는 단계가 있는데 아쉽게도 빅데이터의 전문가라고 얘기하는 사람들이나 실제 프로젝트를 발주하려는 사람들은 단순히 하둡만 도입하면 빅데이터 시스템을 만들 수 있을 것이라고 생각하고 얘기하고 다니는 것을 보게 된다. 물론 다량의 데이터를 수집하고 저장하며 저장된 데이터를 빨리 처리하는 기술이 중요한 것은 사실이지만 어떤 기준으로 그 데이터를 분류하고 처리하게 할 것인가를 알려주는 것이 다름아닌 분석의 단계일진데 그저 데이터를 모으고 처리만 하면 된다고 얘기하는 사람들이 많고 실제로 프로젝트에서 진행되는 모습을 보면 그렇게 접근을 했다가 실패하는 빅데이터 관련 프로젝트들도 많은 것을 보게 된다. 어쩌면 빅데이터 시스템의 핵심은 수집이나 저장, 처리, 결과물 생성보다는 수집, 저장된 데이터를 어떻게 분류하고 데이터간의 연관관계를 찾는지를 연구하는 분석이 아닐까 하는 생각을 해본다. 데이터를 아무리 많이 수집을 해도 그 데이터들 사이의 연관관계를 찾지 못하면 그저 쓰래기 데이터들만 수집하고 저장한 것 밖에는 안된다. 저장소 낭비에 불과하다는 얘기다. 수집된 데이터들 사이의 연관관계를 찾아내는 분석의 단계가 무엇보다도 중요하다.
앞서 얘기했던 것처럼 한국의 빅데이터 시장은 수집과 저장, 그리고 빠른 처리에만 신경을 써왔다는 생각이 든다. 하둡 뿐만이 아니라 오라클, IBM, EMC 등의 다양한 밴더사들이 내놓은 빅데이터 시스템들도 어떻게 보면 하둡처럼 빠른 수집, 저장 및 처리를 내세운 솔루션들이며 이들을 도입하는 것으로 빅데이터 시스템을 도입했다고 생각하는 경우가 많다. 실제로 더 중요한 분석을 등한시 한다는 얘기다. 그리고 그 결과는 앞서 애기했던 것처럼 실무에 적용되지 못하고 그냥 방치되는 솔루션으로 전락하던지 아니면 구축과정 중에서 실패로 끝나는 상황이다.
그나마 최근에는 분석에 대한 이야기들이 나오고 있다. 